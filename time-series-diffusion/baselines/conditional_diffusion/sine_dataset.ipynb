{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Pretraining for Time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import copy\n",
    "os.system(\"unset LD_LIBRARY_PATH\")\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig():\n",
    "    def __init__(self):\n",
    "        self.horizon = 48\n",
    "        self.start_time = 0\n",
    "        self.end_time = 2*np.pi \n",
    "\n",
    "        self.minimum_amplitude = 0.1\n",
    "        self.maximum_amplitude = 1.0\n",
    "        self.amplitude_interval = 0.05\n",
    "        self.amplitudes = list(np.arange(self.minimum_amplitude, self.maximum_amplitude + self.amplitude_interval, self.amplitude_interval))\n",
    "    \n",
    "        self.minimum_frequency = 0.1\n",
    "        self.maximum_frequency = 1 \n",
    "        self.frequency_interval = 0.05\n",
    "        self.frequencies = list(np.arange(self.minimum_frequency, self.maximum_frequency + self.frequency_interval, self.frequency_interval))\n",
    "\n",
    "        self.minimum_phase = 0.0 \n",
    "        self.maximum_phase = 2*np.pi\n",
    "        self.phase_interval = 5*np.pi/180\n",
    "        self.phases = list(np.arange(self.minimum_phase, self.maximum_phase, self.phase_interval))\n",
    "\n",
    "        self.sine_params = list(product(self.amplitudes, self.frequencies, self.phases))\n",
    "\n",
    "        self.train_test_ratio = 0.95\n",
    "        \n",
    "\n",
    "class Sinusoidal():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        time_stamps = np.linspace(self.config.start_time, self.config.end_time, self.config.horizon)\n",
    "        data = [params[0]*np.sin(2*np.pi*params[1]*time_stamps + params[2]) for params in self.config.sine_params]\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.labels = np.array([np.array(params) for params in self.config.sine_params])\n",
    "\n",
    "        self.data = torch.tensor(self.data).type(torch.FloatTensor)\n",
    "        self.data = self.data.unsqueeze(-1)\n",
    "        self.labels = torch.tensor(self.labels).type(torch.FloatTensor)\n",
    "\n",
    "        indices = torch.randperm(self.data.size(0))\n",
    "        num_training_indices = int(self.data.size(0)*self.config.train_test_ratio)\n",
    "        train_indices = indices[:num_training_indices]\n",
    "        test_indices = indices[num_training_indices:]\n",
    "        self.train_data = self.data[train_indices]\n",
    "        self.train_labels = self.labels[train_indices]\n",
    "        self.test_data = self.data[test_indices]\n",
    "        self.test_labels = self.labels[test_indices]\n",
    "\n",
    "    \n",
    "class GetDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.dataset = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, org_index):\n",
    "        return {\"time_series\": self.dataset[org_index], \"labels\": self.labels[org_index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTSPConfig():\n",
    "    def __init__(self):\n",
    "        # train config parameters\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 2000\n",
    "        self.learning_rate = 1e-5\n",
    "        self.n_plots = 6\n",
    "\n",
    "        # model parameters\n",
    "        self.n_features = 1\n",
    "        self.n_params = 3\n",
    "        self.embedding_dim = 128\n",
    "\n",
    "        # model initialization \n",
    "        self.pretrained_loc = 'save/sine_20230810_151623/results/weights/best_model.pth'\n",
    "\n",
    "class TSEncoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, hidden_layer_output_dim=16, num_layers=4, bidirectional=True):\n",
    "        super(TSEncoder, self).__init__()\n",
    "\n",
    "        self.seq_len, self.n_features = seq_len, n_features\n",
    "        self.hidden_layer_output_dim = hidden_layer_output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.flat_dim = self.num_layers*2*self.hidden_layer_output_dim if self.bidirectional else self.num_layers*self.hidden_layer_output_dim\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=n_features, # 1\n",
    "            hidden_size=32, # 32\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.mean = nn.LSTM(\n",
    "            input_size=32*2, \n",
    "            hidden_size=self.hidden_layer_output_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.var = nn.LSTM(\n",
    "            input_size=32*2, \n",
    "            hidden_size=self.hidden_layer_output_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (_, _) = self.rnn1(x) # the shape of x is 1x140x32\n",
    "        _, (mu, _) = self.mean(x)\n",
    "        _, (sigma, _) = self.var(x)\n",
    "        mu = mu.permute(1,0,2).reshape(-1, self.flat_dim)\n",
    "        sigma = sigma.permute(1,0,2).reshape(-1, self.flat_dim)\n",
    "        return mu, sigma\n",
    "\n",
    "class TSDecoder(nn.Module):\n",
    "    def __init__(self, seq_len, hidden_layer_output_dim=64, n_features=1, num_layers=4, bidirectional=True):\n",
    "        super(TSDecoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional \n",
    "        self.input_size = self.num_layers*2*hidden_layer_output_dim if self.bidirectional else self.num_layers*hidden_layer_output_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.rnn1 = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=32,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        self.rnn2 = nn.LSTM(\n",
    "            input_size=32*2,\n",
    "            hidden_size=hidden_layer_output_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        self.rnn3 = nn.LSTM(\n",
    "            input_size=hidden_layer_output_dim*2,\n",
    "            hidden_size=1,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.repeat(1, self.seq_len, 1)\n",
    "        x, (_, _) = self.rnn1(x)\n",
    "        x, (_, _) = self.rnn2(x)\n",
    "        x, (_, _) = self.rnn3(x)\n",
    "        return x\n",
    "\n",
    "class ParamEncoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ParamEncoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, int(output_size // 2))\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(int(output_size // 2), output_size)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.act1(self.fc1(x)))\n",
    "        x = self.dropout(self.act2(self.fc2(x)))\n",
    "        return x\n",
    "\n",
    "class CLTSPRecurrentAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, n_params, embedding_dim=64):\n",
    "        super(CLTSPRecurrentAutoencoder, self).__init__()\n",
    "\n",
    "        self.num_layers = 4\n",
    "        self.bidirectional = True \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_layer_output_dim = int(embedding_dim / (self.num_layers*2)) if self.bidirectional else int(embedding_dim / self.num_layers)\n",
    "        \n",
    "        self.ts_encoder = TSEncoder(seq_len, n_features, self.hidden_layer_output_dim, self.num_layers, self.bidirectional)\n",
    "        self.ts_decoder = TSDecoder(seq_len, self.hidden_layer_output_dim, n_features, self.num_layers, self.bidirectional)\n",
    "        self.param_encoder = ParamEncoder(input_size=n_params, output_size=embedding_dim)\n",
    "\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var)        \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, timeseries, parameters):\n",
    "        mu, log_var = self.ts_encoder(timeseries)\n",
    "        z = self.reparameterization(mu, torch.exp(0.5 * log_var))\n",
    "        reconstruced = self.ts_decoder(z)\n",
    "        \n",
    "        label_projection = self.param_encoder(parameters)\n",
    "        timeseries_projection = z\n",
    "\n",
    "        return reconstruced, mu, log_var, label_projection, timeseries_projection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig() \n",
    "cltsp_config = CLTSPConfig()\n",
    "\n",
    "sine_dataset_obj = Sinusoidal(config=dataset_config)\n",
    "train_dataset = GetDataset(data=sine_dataset_obj.train_data, labels=sine_dataset_obj.train_labels)\n",
    "test_dataset = GetDataset(data=sine_dataset_obj.test_data, labels=sine_dataset_obj.test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=cltsp_config.batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=cltsp_config.batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLTSPRecurrentAutoencoder(seq_len=dataset_config.horizon, n_features=cltsp_config.n_features, n_params=cltsp_config.n_params, embedding_dim=cltsp_config.embedding_dim)\n",
    "model = model.to(device)\n",
    "if cltsp_config.pretrained_loc != '':\n",
    "    model.load_state_dict(torch.load(cltsp_config.pretrained_loc))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, val_dataloader, num_plots, log_dir, epoch, device):\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=num_plots,\n",
    "        sharey=True,\n",
    "        sharex=True,\n",
    "        figsize=(32, 8)\n",
    "    )\n",
    "\n",
    "    model = model.eval()\n",
    "    for val_batch in val_dataloader:\n",
    "        seq_true = val_batch[\"time_series\"]\n",
    "        seq_true_labels = val_batch[\"labels\"]\n",
    "\n",
    "        seq_true = seq_true.to(device)\n",
    "        seq_true_labels = seq_true_labels.to(device)\n",
    "        predictions, _, _, _, _ = model(seq_true, seq_true_labels)\n",
    "        for i in range(num_plots):\n",
    "            pred = predictions[i].squeeze(-1).cpu().detach().numpy()\n",
    "            true = seq_true[i].squeeze(-1).cpu().numpy()\n",
    "            axs[i].plot(true, label='true')\n",
    "            axs[i].plot(pred, label='reconstructed')\n",
    "        break\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_dir = os.path.join(log_dir, 'qualitative')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_loc = os.path.join(save_dir, str(epoch)+'.png')\n",
    "    plt.savefig(save_loc)\n",
    "    plt.close('all')\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var, label_projection, timeseries_projection, criterion):\n",
    "    reconstruction_loss = criterion(x_hat, x) \n",
    "    kl_divergence_loss = - 0.5 * torch.mean(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    # print(reproduction_loss, KLD)\n",
    "\n",
    "    # Calculating the Loss\n",
    "    logits = (label_projection @ timeseries_projection.T)\n",
    "    labels_similarity = label_projection @ label_projection.T\n",
    "    timeseries_similarity = timeseries_projection @ timeseries_projection.T\n",
    "    targets = F.softmax((labels_similarity + timeseries_similarity) / 2, dim=-1)\n",
    "    labels_loss = cross_entropy(logits, targets, reduction='none')\n",
    "    timeseries_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "    similarity_loss =  (labels_loss + timeseries_loss) / 2.0\n",
    "    similarity_loss = similarity_loss.mean()\n",
    "\n",
    "    return reconstruction_loss, kl_divergence_loss, similarity_loss\n",
    "\n",
    "    \n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, train_config, log_dir, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=train_config.learning_rate)\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000000000.0\n",
    "    n_epochs = train_config.n_epochs\n",
    "\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        model = model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for train_batch in train_dataloader:\n",
    "            seq_true = train_batch[\"time_series\"]\n",
    "            seq_true_labels = train_batch[\"labels\"]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            seq_true = seq_true.to(device)\n",
    "            seq_true_labels = seq_true_labels.to(device)\n",
    "            seq_pred, mu, log_var, label_projection, timeseries_projection = model(seq_true, seq_true_labels)\n",
    "            reconstruction_loss, kldivergence_loss, similarity_loss = loss_function(seq_true, seq_pred, mu, log_var, label_projection, timeseries_projection, criterion)\n",
    "            loss = reconstruction_loss + kldivergence_loss + 0*similarity_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                seq_true = val_batch[\"time_series\"]\n",
    "                seq_true_labels = val_batch[\"labels\"]\n",
    "\n",
    "                seq_true = seq_true.to(device)\n",
    "                seq_true_labels = seq_true_labels.to(device)\n",
    "                seq_pred, mu, log_var, label_projection, timeseries_projection = model(seq_true, seq_true_labels)\n",
    "\n",
    "                reconstruction_loss, kld_loss, similarity_loss = loss_function(seq_true, seq_pred, mu, log_var, label_projection, timeseries_projection, criterion)\n",
    "                val_loss = reconstruction_loss + kld_loss + 0*similarity_loss\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        history['train'].append(train_loss)\n",
    "        history['val'].append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        plot_results(model, val_dataloader, train_config.n_plots, log_dir, epoch, device)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            save_dir = os.path.join(log_dir, 'weights')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            save_loc = os.path.join(save_dir, 'best_model.pth')\n",
    "            torch.save(model.state_dict(), save_loc)        \n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model folder: ./save/sine_20230810_151623/\n",
      "Epoch 0: train loss 401.0679082524591 val loss 389.6884031749907\n",
      "Epoch 1: train loss 398.12937397536837 val loss 390.7028539748419\n",
      "Epoch 2: train loss 397.16652533180354 val loss 391.3607824416388\n",
      "Epoch 3: train loss 397.7034780077366 val loss 390.32673209054127\n",
      "Epoch 4: train loss 395.73431349047723 val loss 391.2378162202381\n",
      "Epoch 5: train loss 394.75840601155176 val loss 388.0047389439174\n",
      "Epoch 6: train loss 392.9559528568248 val loss 386.3335447765532\n",
      "Epoch 7: train loss 391.73895034394735 val loss 388.32173810686385\n",
      "Epoch 8: train loss 391.63297813296936 val loss 384.1501624697731\n",
      "Epoch 9: train loss 390.495041209799 val loss 386.4472020467122\n",
      "Epoch 10: train loss 390.1247392565475 val loss 383.5568578810919\n",
      "Epoch 11: train loss 389.3116261378471 val loss 382.7824133010138\n",
      "Epoch 12: train loss 389.1425994714925 val loss 384.953490847633\n",
      "Epoch 13: train loss 390.593432727873 val loss 386.45772370837983\n",
      "Epoch 14: train loss 388.8949374658456 val loss 385.1201418922061\n",
      "Epoch 15: train loss 388.3666437178696 val loss 384.0905590965634\n",
      "Epoch 16: train loss 387.48806754789206 val loss 385.5259225027902\n",
      "Epoch 17: train loss 387.633404351269 val loss 380.74305361793154\n",
      "Epoch 18: train loss 386.53799193387204 val loss 381.9761555989583\n",
      "Epoch 19: train loss 388.4538434280633 val loss 380.9353496006557\n",
      "Epoch 20: train loss 387.39383875397203 val loss 385.21063341413225\n",
      "Epoch 21: train loss 386.0393154954663 val loss 381.8882271902902\n",
      "Epoch 22: train loss 385.647962501012 val loss 380.06796191987536\n",
      "Epoch 23: train loss 385.38932539391396 val loss 377.1174828665597\n",
      "Epoch 24: train loss 384.8424142629989 val loss 381.77492268880206\n",
      "Epoch 25: train loss 384.1672130841665 val loss 379.38586861746654\n",
      "Epoch 26: train loss 383.2427860714611 val loss 377.6312582833426\n",
      "Epoch 27: train loss 383.3719236541906 val loss 374.82328142438615\n",
      "Epoch 28: train loss 383.261265571871 val loss 378.43819827125185\n",
      "Epoch 29: train loss 383.20932204488645 val loss 375.2263681320917\n",
      "Epoch 30: train loss 383.46199170048374 val loss 378.32523672921315\n",
      "Epoch 31: train loss 382.6444559838488 val loss 380.3839489164807\n",
      "Epoch 32: train loss 384.0280487376791 val loss 378.0518115815662\n",
      "Epoch 33: train loss 383.9083458302552 val loss 380.75893147786456\n",
      "Epoch 34: train loss 382.95566230734397 val loss 380.9293641589937\n",
      "Epoch 35: train loss 383.85383827328064 val loss 380.14237031482514\n",
      "Epoch 36: train loss 383.95909395366135 val loss 378.3635958716983\n",
      "Epoch 37: train loss 383.220086606673 val loss 378.33758617582777\n",
      "Epoch 38: train loss 383.5196806754473 val loss 378.5778336297898\n",
      "Epoch 39: train loss 383.70328920986987 val loss 378.5487293061756\n",
      "Epoch 40: train loss 385.52109239014936 val loss 381.4721352713449\n",
      "Epoch 41: train loss 383.1072515774267 val loss 380.9521437145415\n",
      "Epoch 42: train loss 383.2475657092475 val loss 375.87925865536647\n",
      "Epoch 43: train loss 381.16492474017366 val loss 377.9615958077567\n",
      "Epoch 44: train loss 381.2639451891647 val loss 376.0218109857468\n",
      "Epoch 45: train loss 382.8592138735124 val loss 382.53431011381605\n",
      "Epoch 46: train loss 383.7658897755677 val loss 378.35560244605654\n",
      "Epoch 47: train loss 382.087884932602 val loss 381.4093213762556\n",
      "Epoch 48: train loss 381.7585833455614 val loss 376.01066153390065\n",
      "Epoch 49: train loss 381.33451479580737 val loss 377.93519919259205\n",
      "Epoch 50: train loss 381.1298366408274 val loss 376.55031040736606\n",
      "Epoch 51: train loss 381.21825689602395 val loss 373.6751134963263\n",
      "Epoch 52: train loss 382.2815171474002 val loss 376.4908418201265\n",
      "Epoch 53: train loss 382.1004413347788 val loss 379.0911952427455\n",
      "Epoch 54: train loss 387.49682435347006 val loss 377.64114161900113\n",
      "Epoch 55: train loss 387.91494363577254 val loss 379.3364076160249\n",
      "Epoch 56: train loss 387.16239130682277 val loss 381.9615282331194\n",
      "Epoch 57: train loss 388.97080574628603 val loss 383.6537842523484\n",
      "Epoch 58: train loss 385.5301538180811 val loss 379.47331165132067\n",
      "Epoch 59: train loss 389.507439173565 val loss 378.7039170038132\n",
      "Epoch 60: train loss 386.61028404433495 val loss 382.5220220656622\n",
      "Epoch 61: train loss 388.0960085379645 val loss 379.7539818173363\n",
      "Epoch 62: train loss 387.51092758574015 val loss 380.9434029715402\n",
      "Epoch 63: train loss 385.1866081119201 val loss 377.5297175816127\n",
      "Epoch 64: train loss 383.41343285753317 val loss 378.23167419433594\n",
      "Epoch 65: train loss 383.82690437593607 val loss 374.01206752232144\n",
      "Epoch 66: train loss 383.2585295444943 val loss 376.7349177769252\n",
      "Epoch 67: train loss 382.46020033446 val loss 375.4667936052595\n",
      "Epoch 68: train loss 381.3452106535126 val loss 371.5283867972238\n",
      "Epoch 69: train loss 381.91003568184806 val loss 377.8006235758464\n",
      "Epoch 70: train loss 381.39242696020887 val loss 376.2827384585426\n",
      "Epoch 71: train loss 381.984915540626 val loss 374.41324833461215\n",
      "Epoch 72: train loss 381.30898699982794 val loss 377.08191571916853\n",
      "Epoch 73: train loss 381.6189168030734 val loss 373.1047036307199\n",
      "Epoch 74: train loss 381.5500560226836 val loss 374.2590557280041\n",
      "Epoch 75: train loss 380.097603990624 val loss 374.8046678815569\n",
      "Epoch 76: train loss 380.73180440794 val loss 374.3402557373047\n",
      "Epoch 77: train loss 380.9350149520321 val loss 373.6018538702102\n",
      "Epoch 78: train loss 382.1187450824006 val loss 375.1640348888579\n",
      "Epoch 79: train loss 383.3480012725672 val loss 376.15919349307103\n",
      "Epoch 80: train loss 382.8696881230013 val loss 379.2543131510417\n",
      "Epoch 81: train loss 383.5621968798069 val loss 378.9235164097377\n",
      "Epoch 82: train loss 382.8380024173717 val loss 376.70113990420384\n",
      "Epoch 83: train loss 382.1812774183837 val loss 378.2677459716797\n",
      "Epoch 84: train loss 381.16505527002204 val loss 373.4091858636765\n",
      "Epoch 85: train loss 381.66670582825657 val loss 373.7916982741583\n",
      "Epoch 86: train loss 381.6010167413425 val loss 377.2074679420108\n",
      "Epoch 87: train loss 381.2559321902576 val loss 379.4390978131975\n",
      "Epoch 88: train loss 380.6024960532707 val loss 378.67138526553197\n",
      "Epoch 89: train loss 380.2747373432693 val loss 374.85885947091236\n",
      "Epoch 90: train loss 381.58689026511394 val loss 378.77354031517393\n",
      "Epoch 91: train loss 381.6897414607705 val loss 374.1358337402344\n",
      "Epoch 92: train loss 381.01868679857006 val loss 375.3686261858259\n",
      "Epoch 93: train loss 379.5660459686437 val loss 376.84960501534596\n",
      "Epoch 94: train loss 379.0535308363524 val loss 378.12457784016925\n",
      "Epoch 95: train loss 379.104744866722 val loss 374.6908046177455\n",
      "Epoch 96: train loss 379.3666695708438 val loss 372.9409444899786\n",
      "Epoch 97: train loss 383.46552928865265 val loss 375.89702751522975\n",
      "Epoch 98: train loss 382.1554409284048 val loss 376.16221110026044\n",
      "Epoch 99: train loss 380.06571968355325 val loss 371.9912276495071\n",
      "Epoch 100: train loss 380.73591692830615 val loss 375.1944997878302\n",
      "Epoch 101: train loss 384.0085933072579 val loss 376.9610406784784\n",
      "Epoch 102: train loss 384.67887949572946 val loss 377.73441496349517\n",
      "Epoch 103: train loss 395.14629146595695 val loss 395.0362904866536\n",
      "Epoch 104: train loss 400.70191576073205 val loss 390.8614225841704\n",
      "Epoch 105: train loss 392.97658685081365 val loss 379.43734377906435\n",
      "Epoch 106: train loss 392.47907118723185 val loss 386.1940965198335\n",
      "Epoch 107: train loss 390.46454695469356 val loss 380.5237877255394\n",
      "Epoch 108: train loss 387.2170101027415 val loss 381.21387881324404\n",
      "Epoch 109: train loss 390.78940222547465 val loss 384.4551725841704\n",
      "Epoch 110: train loss 391.7946730697711 val loss 382.98592885335285\n",
      "Epoch 111: train loss 391.5069692344863 val loss 379.5445592971075\n",
      "Epoch 112: train loss 386.70357161605915 val loss 376.29138728550504\n",
      "Epoch 113: train loss 383.11284372596543 val loss 378.2121393112909\n",
      "Epoch 114: train loss 383.01875898133903 val loss 374.5593072800409\n",
      "Epoch 115: train loss 382.79832964487025 val loss 377.3895699637277\n",
      "Epoch 116: train loss 381.0645440452457 val loss 376.72037905738466\n",
      "Epoch 117: train loss 382.90369956851623 val loss 377.7959151495071\n",
      "Epoch 118: train loss 382.34939464509796 val loss 375.5174989246187\n",
      "Epoch 119: train loss 383.2805690468902 val loss 380.24484361921037\n",
      "Epoch 120: train loss 383.89952324585596 val loss 381.3196018763951\n",
      "Epoch 121: train loss 383.4933667751174 val loss 376.8548118954613\n",
      "Epoch 122: train loss 382.51619416073817 val loss 373.228035336449\n",
      "Epoch 123: train loss 382.4828366294426 val loss 376.62472134544737\n",
      "Epoch 124: train loss 381.44714663806974 val loss 376.772331964402\n",
      "Epoch 125: train loss 381.585057866388 val loss 377.46604592459545\n",
      "Epoch 126: train loss 381.07892739093364 val loss 370.4566461472284\n",
      "Epoch 127: train loss 379.2258839187227 val loss 370.37031119210377\n",
      "Epoch 128: train loss 378.4722660835543 val loss 376.10457502092635\n",
      "Epoch 129: train loss 382.17280452115546 val loss 382.4965224493118\n",
      "Epoch 130: train loss 386.79621736990975 val loss 379.29515438988096\n",
      "Epoch 131: train loss 386.2079454333053 val loss 382.0448731921968\n",
      "Epoch 132: train loss 385.9142996041886 val loss 378.6677972702753\n",
      "Epoch 133: train loss 384.43559707878785 val loss 377.70140584309894\n",
      "Epoch 134: train loss 385.75873364068065 val loss 377.7119656517392\n",
      "Epoch 135: train loss 385.7194296090714 val loss 380.5697268531436\n",
      "Epoch 136: train loss 386.591103430239 val loss 378.3244156610398\n",
      "Epoch 137: train loss 386.18241518643237 val loss 383.06891087123324\n",
      "Epoch 138: train loss 385.21304194791327 val loss 374.3950634910947\n",
      "Epoch 139: train loss 383.48776395333243 val loss 376.9124051048642\n",
      "Epoch 140: train loss 382.6677674604821 val loss 372.86019752139134\n",
      "Epoch 141: train loss 382.5228767197367 val loss 374.99381292433964\n",
      "Epoch 142: train loss 381.3331026857999 val loss 373.8064931233724\n",
      "Epoch 143: train loss 380.5595023990295 val loss 374.40677279517763\n",
      "Epoch 144: train loss 382.8087046726998 val loss 377.9516245524089\n",
      "Epoch 145: train loss 383.9196588387761 val loss 375.09525335402714\n",
      "Epoch 146: train loss 384.9349684641151 val loss 378.4903742472331\n",
      "Epoch 147: train loss 383.0578184770179 val loss 374.7166726248605\n",
      "Epoch 148: train loss 382.21699974573954 val loss 374.53803107852025\n",
      "Epoch 149: train loss 382.40721454521537 val loss 374.1549609956287\n",
      "Epoch 150: train loss 383.2898708205149 val loss 375.07994442894346\n",
      "Epoch 151: train loss 384.7324529460057 val loss 381.5053017025902\n",
      "Epoch 152: train loss 388.84645728867287 val loss 386.08926536923366\n",
      "Epoch 153: train loss 391.47199292751174 val loss 382.07437933058964\n",
      "Epoch 154: train loss 390.4521033726826 val loss 380.00883411225817\n",
      "Epoch 155: train loss 392.684156625382 val loss 387.20211210704986\n",
      "Epoch 156: train loss 397.85338066773096 val loss 390.3204527355376\n",
      "Epoch 157: train loss 396.6947166166157 val loss 391.4706348237537\n",
      "Epoch 158: train loss 397.21796824400906 val loss 391.4590410505022\n",
      "Epoch 159: train loss 396.47618537863303 val loss 388.3185050601051\n",
      "Epoch 160: train loss 395.51500460767994 val loss 386.000491187686\n",
      "Epoch 161: train loss 394.53587040876477 val loss 380.1524658203125\n",
      "Epoch 162: train loss 389.81458484076467 val loss 379.6107664562407\n",
      "Epoch 163: train loss 387.61556208318996 val loss 379.3154921758743\n",
      "Epoch 164: train loss 389.2842715564787 val loss 379.471435546875\n",
      "Epoch 165: train loss 398.7086261492319 val loss 391.834957304455\n",
      "Epoch 166: train loss 398.75616281143743 val loss 386.5910121372768\n",
      "Epoch 167: train loss 397.1000740960472 val loss 387.16995638892763\n",
      "Epoch 168: train loss 397.38438375996805 val loss 390.5235072544643\n",
      "Epoch 169: train loss 397.8312234038516 val loss 387.9861624581473\n",
      "Epoch 170: train loss 399.41472914305376 val loss 386.88697451636904\n",
      "Epoch 171: train loss 396.6371931955604 val loss 388.110219682966\n",
      "Epoch 172: train loss 402.0766597609446 val loss 393.5293157668341\n",
      "Epoch 173: train loss 403.36625924382184 val loss 392.09005591982884\n",
      "Epoch 174: train loss 402.32938792174343 val loss 392.57710702078685\n",
      "Epoch 175: train loss 399.7142166374879 val loss 389.3374314081101\n",
      "Epoch 176: train loss 398.3436591588154 val loss 391.3584958031064\n",
      "Epoch 177: train loss 397.69385508799184 val loss 387.1859377906436\n",
      "Epoch 178: train loss 400.99270946127143 val loss 390.46788606189546\n",
      "Epoch 179: train loss 397.0982907151929 val loss 387.7632319132487\n",
      "Epoch 180: train loss 395.62638388520077 val loss 385.6049248831613\n",
      "Epoch 181: train loss 392.63198307017586 val loss 384.7391415550595\n",
      "Epoch 182: train loss 388.0776555352878 val loss 378.4134957449777\n",
      "Epoch 183: train loss 387.40520331031917 val loss 381.3174165998186\n",
      "Epoch 184: train loss 386.62409024411534 val loss 378.54400562104723\n",
      "Epoch 185: train loss 386.7550104961494 val loss 375.4112770443871\n",
      "Epoch 186: train loss 385.5415550587708 val loss 377.43738555908203\n",
      "Epoch 187: train loss 386.2438755331879 val loss 379.17548406691776\n",
      "Epoch 188: train loss 384.57348324474276 val loss 374.5120522635324\n",
      "Epoch 189: train loss 384.2847430767791 val loss 377.4368129911877\n",
      "Epoch 190: train loss 384.37272043178734 val loss 378.01111493791853\n",
      "Epoch 191: train loss 383.4983970365376 val loss 378.53881072998047\n",
      "Epoch 192: train loss 384.40144047712414 val loss 378.49740309942337\n",
      "Epoch 193: train loss 383.09168207830714 val loss 375.3470706031436\n",
      "Epoch 194: train loss 382.5873799694634 val loss 375.661258879162\n",
      "Epoch 195: train loss 382.20917744216524 val loss 372.4423472086589\n",
      "Epoch 196: train loss 381.8177185058594 val loss 373.6448865618025\n",
      "Epoch 197: train loss 381.4519533147466 val loss 371.36096954345703\n",
      "Epoch 198: train loss 382.11929503129556 val loss 373.4732887631371\n",
      "Epoch 199: train loss 380.89403854132934 val loss 373.69858914329893\n",
      "Epoch 200: train loss 381.0307419534792 val loss 374.38252403622585\n",
      "Epoch 201: train loss 380.33093443559244 val loss 372.1214824858166\n",
      "Epoch 202: train loss 378.56218114665137 val loss 371.9567602248419\n",
      "Epoch 203: train loss 379.77936332584045 val loss 371.4947211855934\n",
      "Epoch 204: train loss 378.82196693222755 val loss 370.18775576636904\n",
      "Epoch 205: train loss 376.87178221391275 val loss 372.26729765392486\n",
      "Epoch 206: train loss 377.5840745046349 val loss 369.8480464390346\n",
      "Epoch 207: train loss 378.09559773657605 val loss 374.81872304280597\n",
      "Epoch 208: train loss 386.42034390306225 val loss 384.50039745512464\n",
      "Epoch 209: train loss 389.17131540565293 val loss 379.1911919003441\n",
      "Epoch 210: train loss 385.7962222716969 val loss 377.21527608235675\n",
      "Epoch 211: train loss 384.2762861498897 val loss 377.17901901971726\n",
      "Epoch 212: train loss 384.0421105419416 val loss 375.53746505010696\n",
      "Epoch 213: train loss 383.4523794539852 val loss 376.95750681559247\n",
      "Epoch 214: train loss 383.2172071229609 val loss 373.82427469889325\n",
      "Epoch 215: train loss 381.79707680341494 val loss 382.35460481189546\n",
      "Epoch 216: train loss 384.78684088356135 val loss 377.19581821986606\n",
      "Epoch 217: train loss 383.2141219618407 val loss 375.3201377505348\n",
      "Epoch 218: train loss 382.95829417174343 val loss 374.60028076171875\n",
      "Epoch 219: train loss 381.6617677520594 val loss 375.2859406244187\n",
      "Epoch 220: train loss 380.91720462463064 val loss 375.67239452543714\n",
      "Epoch 221: train loss 382.3171179578712 val loss 378.00516619001115\n",
      "Epoch 222: train loss 379.336257618326 val loss 372.8556097121466\n",
      "Epoch 223: train loss 380.5269818874221 val loss 369.58096095493863\n",
      "Epoch 224: train loss 380.31201669959825 val loss 376.1415710449219\n",
      "Epoch 225: train loss 381.16543452603827 val loss 377.6306413922991\n",
      "Epoch 226: train loss 387.44510211845756 val loss 383.02296665736606\n",
      "Epoch 227: train loss 388.2058716610923 val loss 377.646489461263\n",
      "Epoch 228: train loss 384.4919058053605 val loss 378.4257547287714\n",
      "Epoch 229: train loss 382.675870509963 val loss 377.1055345081148\n",
      "Epoch 230: train loss 382.88775405488485 val loss 375.6759338378906\n",
      "Epoch 231: train loss 383.9753352348051 val loss 374.0895182291667\n",
      "Epoch 232: train loss 383.06976697852576 val loss 375.9787270682199\n",
      "Epoch 233: train loss 381.5487342794942 val loss 377.554448445638\n",
      "Epoch 234: train loss 386.66938947766556 val loss 382.95079912458147\n",
      "Epoch 235: train loss 390.36328860268077 val loss 379.1953582763672\n",
      "Epoch 236: train loss 391.1745770706414 val loss 380.4672411964053\n",
      "Epoch 237: train loss 386.80798695618626 val loss 381.23316374279204\n",
      "Epoch 238: train loss 390.77697635314627 val loss 383.2259703136626\n",
      "Epoch 239: train loss 393.6027620938158 val loss 384.3431785220192\n",
      "Epoch 240: train loss 395.32193198722877 val loss 389.0345709664481\n",
      "Epoch 241: train loss 393.6294677418131 val loss 384.14420136951264\n",
      "Epoch 242: train loss 397.28467611204155 val loss 387.8622556413923\n",
      "Epoch 243: train loss 399.1983984912615 val loss 390.8421609061105\n",
      "Epoch 244: train loss 400.7918617367127 val loss 394.53398967924574\n",
      "Epoch 245: train loss 401.15821798848367 val loss 390.5154691423689\n",
      "Epoch 246: train loss 411.32568240783377 val loss 408.1937749953497\n",
      "Epoch 247: train loss 430.1284603454906 val loss 416.48812284923736\n",
      "Epoch 248: train loss 428.5735127345268 val loss 418.0154742286319\n",
      "Epoch 249: train loss 416.54611941322764 val loss 401.46463448660717\n",
      "Epoch 250: train loss 405.3877089110063 val loss 390.90145074753536\n",
      "Epoch 251: train loss 408.1692101671288 val loss 398.6451136271159\n",
      "Epoch 252: train loss 405.1231072776676 val loss 391.6954803466797\n",
      "Epoch 253: train loss 397.1679379952386 val loss 391.09791455950057\n",
      "Epoch 254: train loss 416.48242701397044 val loss 412.02539716448103\n",
      "Epoch 255: train loss 422.43785000341546 val loss 398.55413891020277\n",
      "Epoch 256: train loss 404.62179391495306 val loss 392.09994143531435\n",
      "Epoch 257: train loss 402.0255617131841 val loss 391.34249223981584\n",
      "Epoch 258: train loss 403.6633218557723 val loss 392.9845079694475\n",
      "Epoch 259: train loss 395.7365650710664 val loss 381.7327168782552\n",
      "Epoch 260: train loss 391.0452945689463 val loss 380.67354402087983\n",
      "Epoch 261: train loss 387.5769633555042 val loss 381.29750061035156\n",
      "Epoch 262: train loss 387.557927818496 val loss 381.7018556140718\n",
      "Epoch 263: train loss 389.5712637629534 val loss 381.255612327939\n",
      "Epoch 264: train loss 388.20804425965935 val loss 379.45677185058594\n",
      "Epoch 265: train loss 385.1870890404894 val loss 377.2985098702567\n",
      "Epoch 266: train loss 383.8709796648569 val loss 376.2591262090774\n",
      "Epoch 267: train loss 383.8789509985731 val loss 379.3618454706101\n",
      "Epoch 268: train loss 386.3972454960482 val loss 379.645030430385\n",
      "Epoch 269: train loss 387.3730743091959 val loss 383.72837974911647\n",
      "Epoch 270: train loss 391.06459606743846 val loss 385.2978951590402\n",
      "Epoch 271: train loss 394.08751089461725 val loss 386.92883736746654\n",
      "Epoch 272: train loss 407.50750258055376 val loss 416.7553500220889\n",
      "Epoch 273: train loss 427.66404297191247 val loss 420.7996306646438\n",
      "Epoch 274: train loss 423.4281627279489 val loss 407.13866061256044\n",
      "Epoch 275: train loss 420.37699320896917 val loss 411.555184500558\n",
      "Epoch 276: train loss 414.4008846777091 val loss 397.61167290097194\n",
      "Epoch 277: train loss 405.3884563544871 val loss 398.2294751121884\n",
      "Epoch 278: train loss 412.6911107196709 val loss 403.79817635672435\n",
      "Epoch 279: train loss 409.5560145402819 val loss 398.27136666434154\n",
      "Epoch 280: train loss 409.3998322956303 val loss 398.3363574800037\n",
      "Epoch 281: train loss 416.41108838017124 val loss 403.4183883666992\n",
      "Epoch 282: train loss 412.2441618133703 val loss 400.80215599423366\n",
      "Epoch 283: train loss 419.6866016289113 val loss 412.1906978062221\n",
      "Epoch 284: train loss 417.6271712545286 val loss 406.87666393461683\n",
      "Epoch 285: train loss 418.6304734778528 val loss 398.7135235014416\n",
      "Epoch 286: train loss 407.97492585651617 val loss 395.39961315336683\n",
      "Epoch 287: train loss 412.32301109195373 val loss 402.0844217936198\n",
      "Epoch 288: train loss 415.937501897466 val loss 401.4598853701637\n",
      "Epoch 289: train loss 407.86330591705797 val loss 393.7574927920387\n",
      "Epoch 290: train loss 406.68761811725835 val loss 392.91287413097564\n",
      "Epoch 291: train loss 406.503664006841 val loss 398.2858599708194\n",
      "Epoch 292: train loss 410.6776666196517 val loss 401.9066859654018\n",
      "Epoch 293: train loss 418.7177052077852 val loss 410.96790931338353\n",
      "Epoch 294: train loss 425.09342774702475 val loss 416.34489077613466\n",
      "Epoch 295: train loss 420.7031533038678 val loss 406.82236844017393\n",
      "Epoch 296: train loss 417.289073884796 val loss 400.69024476550874\n",
      "Epoch 297: train loss 402.1845963235964 val loss 393.71281287783665\n",
      "Epoch 298: train loss 401.16070849166636 val loss 387.14384387788317\n",
      "Epoch 299: train loss 400.7857802791299 val loss 390.2813742501395\n",
      "Epoch 300: train loss 400.780270591301 val loss 393.0116003127325\n",
      "Epoch 301: train loss 401.5062133314696 val loss 387.99522000267393\n",
      "Epoch 302: train loss 398.6342707816801 val loss 388.4237096877325\n",
      "Epoch 303: train loss 393.49484956573326 val loss 387.70105707077755\n",
      "Epoch 304: train loss 395.0559354001376 val loss 390.4970194498698\n",
      "Epoch 305: train loss 403.19658957367733 val loss 401.2581111363002\n",
      "Epoch 306: train loss 411.5219029243746 val loss 402.44312068394254\n",
      "Epoch 307: train loss 411.6000538564099 val loss 402.58354622977123\n",
      "Epoch 308: train loss 413.5839019142902 val loss 404.09556543259396\n",
      "Epoch 309: train loss 415.11587698348444 val loss 406.8375752766927\n",
      "Epoch 310: train loss 414.4574793682197 val loss 403.1816188267299\n",
      "Epoch 311: train loss 420.7090473866833 val loss 411.8633001418341\n",
      "Epoch 312: train loss 421.9211077912484 val loss 413.62789771670384\n",
      "Epoch 313: train loss 420.1731791916289 val loss 411.4445328485398\n",
      "Epoch 314: train loss 419.3990157527627 val loss 409.8236832391648\n",
      "Epoch 315: train loss 420.4341496284762 val loss 414.45431227911087\n",
      "Epoch 316: train loss 423.341247795777 val loss 437.0757024855841\n",
      "Epoch 317: train loss 442.6023070972818 val loss 430.2121778215681\n",
      "Epoch 318: train loss 435.93194232209356 val loss 425.86212957473026\n",
      "Epoch 319: train loss 431.6037315408183 val loss 416.13409641810824\n",
      "Epoch 320: train loss 431.01487241754876 val loss 418.43004826136996\n",
      "Epoch 321: train loss 430.67731503501454 val loss 423.60824221656435\n",
      "Epoch 322: train loss 440.31473300123463 val loss 433.46036202566967\n",
      "Epoch 323: train loss 440.3698566812308 val loss 431.4718882242839\n",
      "Epoch 324: train loss 441.71626447766556 val loss 433.82044837588353\n",
      "Epoch 325: train loss 440.478218276266 val loss 428.0275384812128\n",
      "Epoch 326: train loss 427.5370697653973 val loss 413.9297667003813\n",
      "Epoch 327: train loss 429.20649869454337 val loss 422.11304873511904\n",
      "Epoch 328: train loss 431.09125190695335 val loss 427.26485915411087\n",
      "Epoch 329: train loss 432.02690883498116 val loss 419.33472115652904\n",
      "Epoch 330: train loss 445.76058034946266 val loss 437.12879071916853\n",
      "Epoch 331: train loss 448.4612663941062 val loss 436.3642251150949\n",
      "Epoch 332: train loss 456.5481540502044 val loss 441.61045328776044\n",
      "Epoch 333: train loss 446.8513625545205 val loss 430.50519598098026\n",
      "Epoch 334: train loss 444.3459940697863 val loss 431.5119360060919\n",
      "Epoch 335: train loss 435.6510304663465 val loss 420.51900954473587\n",
      "Epoch 336: train loss 430.72010858565415 val loss 419.95236424037387\n",
      "Epoch 337: train loss 428.4416088835563 val loss 418.4615485781715\n",
      "Epoch 338: train loss 424.4902093916977 val loss 413.39107876732237\n",
      "Epoch 339: train loss 423.8285463955736 val loss 422.8573288690476\n",
      "Epoch 340: train loss 426.0915691790803 val loss 410.36742401123047\n",
      "Epoch 341: train loss 420.3527903976836 val loss 416.0323733375186\n",
      "Epoch 342: train loss 418.47661649871986 val loss 404.41299801781065\n",
      "Epoch 343: train loss 414.3384541724012 val loss 401.3741186232794\n",
      "Epoch 344: train loss 407.00067486540644 val loss 396.9237118675595\n",
      "Epoch 345: train loss 408.2299729579471 val loss 399.6290817260742\n",
      "Epoch 346: train loss 420.38406356258093 val loss 414.10202244349887\n",
      "Epoch 347: train loss 421.19464822877876 val loss 409.62967209588913\n",
      "Epoch 348: train loss 424.10789537182745 val loss 415.5256943475632\n",
      "Epoch 349: train loss 426.3760336446021 val loss 416.7557409377325\n",
      "Epoch 350: train loss 426.1885171208357 val loss 416.8313293457031\n",
      "Epoch 351: train loss 426.6773144025259 val loss 417.80749657040553\n",
      "Epoch 352: train loss 434.35515448219417 val loss 428.2787108648391\n",
      "Epoch 353: train loss 437.20891021570395 val loss 420.39540027436755\n",
      "Epoch 354: train loss 433.54714483547707 val loss 422.74562109084354\n",
      "Epoch 355: train loss 432.3963222997794 val loss 414.5043516613188\n",
      "Epoch 356: train loss 421.40753276607535 val loss 412.08166067940846\n",
      "Epoch 357: train loss 420.93838967436955 val loss 409.25531296502976\n",
      "Epoch 358: train loss 416.41210399884636 val loss 404.5791186378116\n",
      "Epoch 359: train loss 414.5376069063967 val loss 403.89308275495256\n",
      "Epoch 360: train loss 415.2305483645108 val loss 402.1980147588821\n",
      "Epoch 361: train loss 413.1041504064372 val loss 402.6910607474191\n",
      "Epoch 362: train loss 410.9056284217637 val loss 403.2442408970424\n",
      "Epoch 363: train loss 410.3611326860023 val loss 398.2102355957031\n",
      "Epoch 364: train loss 404.1717811544942 val loss 393.51541936965214\n",
      "Epoch 365: train loss 401.57001119821183 val loss 394.5371842157273\n",
      "Epoch 366: train loss 401.225787306079 val loss 394.3267771402995\n",
      "Epoch 367: train loss 399.71734619140625 val loss 390.3521259852818\n",
      "Epoch 368: train loss 398.4615332252621 val loss 388.08953239804225\n",
      "Epoch 369: train loss 396.5282534268236 val loss 388.5221593947638\n",
      "Epoch 370: train loss 398.7662702175002 val loss 389.85636029924666\n",
      "Epoch 371: train loss 397.75254505533013 val loss 386.2752656482515\n",
      "Epoch 372: train loss 398.1599865058543 val loss 390.0322832380022\n",
      "Epoch 373: train loss 401.1903996442884 val loss 394.2754945300874\n",
      "Epoch 374: train loss 398.6715536957578 val loss 388.24290756952195\n",
      "Epoch 375: train loss 396.3337578649966 val loss 386.5222981770833\n",
      "Epoch 376: train loss 397.1484766352362 val loss 387.45498729887464\n",
      "Epoch 377: train loss 403.3686679187834 val loss 395.03631809779574\n",
      "Epoch 378: train loss 402.0570420971806 val loss 391.5871008010138\n",
      "Epoch 379: train loss 401.452076254731 val loss 391.1046458653041\n",
      "Epoch 380: train loss 407.26196874114515 val loss 394.46683175223217\n",
      "Epoch 381: train loss 403.6791661712172 val loss 390.73667689732144\n",
      "Epoch 382: train loss 404.6132980109496 val loss 395.2267339797247\n",
      "Epoch 383: train loss 405.3719511674476 val loss 394.776499067034\n",
      "Epoch 384: train loss 402.50354881484276 val loss 392.69956970214844\n",
      "Epoch 385: train loss 405.96623633191996 val loss 388.21111479259673\n",
      "Epoch 386: train loss 396.44969493490424 val loss 389.78304763067337\n",
      "Epoch 387: train loss 395.37951391348565 val loss 385.55245681036087\n",
      "Epoch 388: train loss 393.6788890621205 val loss 386.2730763753255\n",
      "Epoch 389: train loss 397.3500703011152 val loss 390.3675228300549\n",
      "Epoch 390: train loss 399.17261991352615 val loss 389.964722769601\n",
      "Epoch 391: train loss 400.0798443413769 val loss 390.2169545491536\n",
      "Epoch 392: train loss 399.800126703292 val loss 390.04605683826264\n",
      "Epoch 393: train loss 402.11733898103546 val loss 389.115113394601\n",
      "Epoch 394: train loss 395.0134672649166 val loss 384.92134057907833\n",
      "Epoch 395: train loss 393.587884932602 val loss 386.33521888369603\n",
      "Epoch 396: train loss 392.9406464729902 val loss 384.6659313383557\n",
      "Epoch 397: train loss 392.06488384978144 val loss 384.5147080194382\n",
      "Epoch 398: train loss 391.14810386222877 val loss 383.41668773832777\n",
      "Epoch 399: train loss 388.6260855086727 val loss 379.15848577590214\n",
      "Epoch 400: train loss 391.58127495167787 val loss 385.73954119001115\n",
      "Epoch 401: train loss 398.79670881360306 val loss 394.2385515485491\n",
      "Epoch 402: train loss 405.13125926595893 val loss 395.32330249604723\n",
      "Epoch 403: train loss 405.7388001278892 val loss 390.8132702055432\n",
      "Epoch 404: train loss 399.90392252077095 val loss 390.45504869733537\n",
      "Epoch 405: train loss 401.43343072604637 val loss 390.2040710449219\n",
      "Epoch 406: train loss 406.2350421174203 val loss 401.2102290562221\n",
      "Epoch 407: train loss 412.56570513631397 val loss 401.26294998895554\n",
      "Epoch 408: train loss 411.9394739180649 val loss 400.03577604747954\n",
      "Epoch 409: train loss 405.61694675900156 val loss 388.0449378603981\n",
      "Epoch 410: train loss 397.79462942805316 val loss 392.32712809244794\n",
      "Epoch 411: train loss 397.4359158530754 val loss 385.42845226469495\n",
      "Epoch 412: train loss 396.24842905627634 val loss 386.7491186232794\n",
      "Epoch 413: train loss 396.7502065866105 val loss 387.8304683140346\n",
      "Epoch 414: train loss 397.5515194433341 val loss 386.8182115100679\n",
      "Epoch 415: train loss 397.229678376351 val loss 388.95779309953963\n",
      "Epoch 416: train loss 401.8289555366793 val loss 391.7742295038132\n",
      "Epoch 417: train loss 401.5493752276959 val loss 391.0557621547154\n",
      "Epoch 418: train loss 401.8973854341655 val loss 394.95430864606584\n",
      "Epoch 419: train loss 405.14103983350367 val loss 401.2331085205078\n",
      "Epoch 420: train loss 407.5801029501801 val loss 397.1131119501023\n",
      "Epoch 421: train loss 411.14322769085976 val loss 403.28021458217074\n",
      "Epoch 422: train loss 412.6702204887113 val loss 398.70994567871094\n",
      "Epoch 423: train loss 408.8985010651109 val loss 393.6165942237491\n",
      "Epoch 424: train loss 409.5999417477939 val loss 398.8641873314267\n",
      "Epoch 425: train loss 408.4065696439595 val loss 400.13941446940106\n",
      "Epoch 426: train loss 407.8843108656493 val loss 393.7192949567522\n",
      "Epoch 427: train loss 401.72227248750204 val loss 387.0714398338681\n",
      "Epoch 428: train loss 398.7747340227038 val loss 389.6086647396996\n",
      "Epoch 429: train loss 397.3041512286725 val loss 390.1113063267299\n",
      "Epoch 430: train loss 401.35270611856885 val loss 393.50388881138394\n",
      "Epoch 431: train loss 399.34130187355794 val loss 389.17250170026506\n",
      "Epoch 432: train loss 399.20215990135705 val loss 394.33299146379744\n",
      "Epoch 433: train loss 404.837115431079 val loss 392.9922129313151\n",
      "Epoch 434: train loss 399.6551108088518 val loss 389.9104734148298\n",
      "Epoch 435: train loss 406.1035670147041 val loss 403.8138122558594\n",
      "Epoch 436: train loss 409.32011397762 val loss 397.29996890113466\n",
      "Epoch 437: train loss 409.6708506055447 val loss 401.35221136183964\n",
      "Epoch 438: train loss 400.5115320867825 val loss 389.4719812302362\n",
      "Epoch 439: train loss 399.67287820608504 val loss 392.35176631382535\n",
      "Epoch 440: train loss 400.949041099746 val loss 389.66754731677827\n",
      "Epoch 441: train loss 395.37728280971703 val loss 387.5375482468378\n",
      "Epoch 442: train loss 393.48766315045134 val loss 383.7634066627139\n",
      "Epoch 443: train loss 393.07001499314384 val loss 384.5640233357747\n",
      "Epoch 444: train loss 400.2643170134391 val loss 390.6021968296596\n",
      "Epoch 445: train loss 397.34652196063894 val loss 387.7187957763672\n",
      "Epoch 446: train loss 395.2106659251791 val loss 382.5069638206845\n",
      "Epoch 447: train loss 391.16293461458673 val loss 383.98463221958707\n",
      "Epoch 448: train loss 390.5312367967991 val loss 385.78958674839566\n",
      "Epoch 449: train loss 390.5464520232047 val loss 384.5015440441313\n",
      "Epoch 450: train loss 391.2685172916076 val loss 387.51307896205356\n",
      "Epoch 451: train loss 393.3884962012731 val loss 386.95605032784596\n",
      "Epoch 452: train loss 396.4860949738656 val loss 386.6728511991955\n",
      "Epoch 453: train loss 399.93799990322924 val loss 391.6564432779948\n",
      "Epoch 454: train loss 398.7239103169021 val loss 386.57762945265995\n",
      "Epoch 455: train loss 396.7303522930244 val loss 388.68789963495163\n",
      "Epoch 456: train loss 399.80200827801167 val loss 389.1616886683873\n",
      "Epoch 457: train loss 395.5104180370588 val loss 385.5763575236003\n",
      "Epoch 458: train loss 392.6632684104801 val loss 382.55067298525853\n",
      "Epoch 459: train loss 392.7145581023063 val loss 383.82315136137464\n",
      "Epoch 460: train loss 394.7110968080827 val loss 385.1233389718192\n",
      "Epoch 461: train loss 398.39969378061244 val loss 389.5922139485677\n",
      "Epoch 462: train loss 397.27142800444767 val loss 391.99787176223026\n",
      "Epoch 463: train loss 397.51940562193874 val loss 392.53739856538317\n",
      "Epoch 464: train loss 402.9946905738949 val loss 392.08310844784694\n",
      "Epoch 465: train loss 395.17762163389534 val loss 386.07913789295014\n",
      "Epoch 466: train loss 394.83140476997653 val loss 386.4824534824916\n",
      "Epoch 467: train loss 399.61213810579767 val loss 399.0012999035063\n",
      "Epoch 468: train loss 411.28016140794506 val loss 400.1214261736189\n",
      "Epoch 469: train loss 409.9270867066062 val loss 397.79424830845426\n",
      "Epoch 470: train loss 406.3666275894086 val loss 397.6754746210007\n",
      "Epoch 471: train loss 405.9879861149763 val loss 398.1234101795015\n",
      "Epoch 472: train loss 406.10298741790297 val loss 398.62222362699964\n",
      "Epoch 473: train loss 409.4264625193542 val loss 396.31478227887834\n",
      "Epoch 474: train loss 407.27139464067056 val loss 398.8921625046503\n",
      "Epoch 475: train loss 400.38563118573916 val loss 384.3290419805618\n",
      "Epoch 476: train loss 392.60086573467356 val loss 386.1410151890346\n",
      "Epoch 477: train loss 391.0120001283952 val loss 383.98535701206754\n",
      "Epoch 478: train loss 393.3328292925741 val loss 384.292715890067\n",
      "Epoch 479: train loss 393.9077436219843 val loss 384.99303763253346\n",
      "Epoch 480: train loss 397.61123174953957 val loss 391.7788514636812\n",
      "Epoch 481: train loss 401.5239779615649 val loss 395.0550515311105\n",
      "Epoch 482: train loss 397.81060126902526 val loss 387.8705636887323\n",
      "Epoch 483: train loss 397.6889798653558 val loss 391.07055119105746\n",
      "Epoch 484: train loss 400.26463539242127 val loss 388.50330752418154\n",
      "Epoch 485: train loss 398.77187564830086 val loss 386.7638676961263\n",
      "Epoch 486: train loss 404.1842286104983 val loss 395.2086719331287\n",
      "Epoch 487: train loss 403.74805169772605 val loss 396.6808765956334\n",
      "Epoch 488: train loss 402.04578830541107 val loss 389.4842329479399\n",
      "Epoch 489: train loss 394.08814915474215 val loss 384.9514389038086\n",
      "Epoch 490: train loss 392.7769258331141 val loss 385.83660307384673\n",
      "Epoch 491: train loss 393.65072742521454 val loss 384.76744152250745\n",
      "Epoch 492: train loss 392.0864483136587 val loss 381.96781340099517\n",
      "Epoch 493: train loss 386.63397129829684 val loss 378.01708039783296\n",
      "Epoch 494: train loss 385.7645696926611 val loss 379.73695409865604\n",
      "Epoch 495: train loss 383.90020277586626 val loss 374.004646664574\n",
      "Epoch 496: train loss 382.6419801860276 val loss 374.0364496140253\n",
      "Epoch 497: train loss 387.40051957362675 val loss 381.65311250232514\n",
      "Epoch 498: train loss 389.386105077872 val loss 383.2488032749721\n",
      "Epoch 499: train loss 390.6653757836535 val loss 381.721188499814\n",
      "Epoch 500: train loss 391.53638596608846 val loss 384.34632364908856\n",
      "Epoch 501: train loss 392.593263616216 val loss 386.5809137253534\n",
      "Epoch 502: train loss 391.8768199070748 val loss 381.3082304454985\n",
      "Epoch 503: train loss 388.91579796983785 val loss 381.2883061000279\n",
      "Epoch 504: train loss 389.2388919178068 val loss 382.9735892159598\n",
      "Epoch 505: train loss 389.9292330568936 val loss 377.3679388137091\n",
      "Epoch 506: train loss 382.44127358550236 val loss 377.34247225806826\n",
      "Epoch 507: train loss 383.0781208097626 val loss 378.4460638137091\n",
      "Epoch 508: train loss 384.4250836150016 val loss 378.8426015944708\n",
      "Epoch 509: train loss 384.7968928678048 val loss 380.395515805199\n",
      "Epoch 510: train loss 386.78650732485124 val loss 378.7462703159877\n",
      "Epoch 511: train loss 387.1185852999514 val loss 378.82655734107607\n",
      "Epoch 512: train loss 386.637929965795 val loss 379.74276370093935\n",
      "Epoch 513: train loss 387.2155126067641 val loss 379.6204507010324\n",
      "Epoch 514: train loss 388.6041203632256 val loss 380.38253493536087\n",
      "Epoch 515: train loss 388.22958951169346 val loss 381.23958405994233\n",
      "Epoch 516: train loss 398.48551055433836 val loss 391.50206284295945\n",
      "Epoch 517: train loss 401.07231582878785 val loss 391.17218598865327\n",
      "Epoch 518: train loss 400.7761272371124 val loss 392.14096977597194\n",
      "Epoch 519: train loss 397.7364928092364 val loss 387.9196261451358\n",
      "Epoch 520: train loss 397.0169206530319 val loss 390.59448678152904\n",
      "Epoch 521: train loss 396.60395994828775 val loss 385.92298816499255\n",
      "Epoch 522: train loss 405.4492951230064 val loss 395.30450221470426\n",
      "Epoch 523: train loss 404.31491760886394 val loss 395.42255147298175\n",
      "Epoch 524: train loss 407.0065565356319 val loss 398.57716587611606\n",
      "Epoch 525: train loss 406.8914312649267 val loss 395.8718000139509\n",
      "Epoch 526: train loss 410.7387428086039 val loss 399.4207476661319\n",
      "Epoch 527: train loss 422.92708836688894 val loss 422.73224966866627\n",
      "Epoch 528: train loss 428.4522362743635 val loss 404.82645016624815\n",
      "Epoch 529: train loss 416.20433898293294 val loss 409.43523842947826\n",
      "Epoch 530: train loss 419.8228264052633 val loss 405.70435987200057\n",
      "Epoch 531: train loss 417.024606892482 val loss 402.77384294782365\n",
      "Epoch 532: train loss 414.9798685182561 val loss 405.1269349597749\n",
      "Epoch 533: train loss 414.15473724523355 val loss 399.98473866780597\n",
      "Epoch 534: train loss 410.37537498671776 val loss 403.57815842401413\n",
      "Epoch 535: train loss 409.7107920276069 val loss 399.6905038016183\n",
      "Epoch 536: train loss 407.10935310007994 val loss 397.3774581182571\n",
      "Epoch 537: train loss 402.95092559972574 val loss 391.1141124906994\n",
      "Epoch 538: train loss 413.17509523698084 val loss 405.6250523158482\n",
      "Epoch 539: train loss 420.71673109617876 val loss 401.58489481608075\n",
      "Epoch 540: train loss 404.6257536893064 val loss 393.40647851853146\n",
      "Epoch 541: train loss 398.911167045949 val loss 386.71966552734375\n",
      "Epoch 542: train loss 395.2846393486379 val loss 387.144655136835\n",
      "Epoch 543: train loss 395.3889651125577 val loss 389.69454411097934\n",
      "Epoch 544: train loss 395.18663617860466 val loss 386.0104860578264\n",
      "Epoch 545: train loss 395.9838275019987 val loss 385.81944565545945\n",
      "Epoch 546: train loss 399.7321212057005 val loss 390.37391553606307\n",
      "Epoch 547: train loss 399.8271063770037 val loss 389.48640805199034\n",
      "Epoch 548: train loss 398.66867215645743 val loss 389.8553684779576\n",
      "Epoch 549: train loss 396.8376765275866 val loss 383.9921079363142\n",
      "Epoch 550: train loss 390.6981110251629 val loss 383.3849639892578\n",
      "Epoch 551: train loss 390.3200355490255 val loss 383.9233725411551\n",
      "Epoch 552: train loss 387.7390508305841 val loss 378.4822827293759\n",
      "Epoch 553: train loss 386.1739545436721 val loss 376.1354064941406\n",
      "Epoch 554: train loss 390.5270218923302 val loss 382.699462890625\n",
      "Epoch 555: train loss 390.26698311128763 val loss 382.62940179734005\n",
      "Epoch 556: train loss 383.917964006335 val loss 376.2605895996094\n",
      "Epoch 557: train loss 383.28217145692497 val loss 373.87191082182386\n",
      "Epoch 558: train loss 382.4440292595582 val loss 376.2223405383882\n",
      "Epoch 559: train loss 382.2128058715188 val loss 372.7314947219122\n",
      "Epoch 560: train loss 382.27133202429263 val loss 379.0996460687546\n",
      "Epoch 561: train loss 387.2865274854275 val loss 385.9730493454706\n",
      "Epoch 562: train loss 389.39687707140035 val loss 381.5833562215169\n",
      "Epoch 563: train loss 389.405262052704 val loss 382.4540706816174\n",
      "Epoch 564: train loss 389.4407288546389 val loss 379.55224173409596\n",
      "Epoch 565: train loss 387.26006993110934 val loss 377.01212238130114\n",
      "Epoch 566: train loss 384.7206844665844 val loss 380.4323962983631\n",
      "Epoch 567: train loss 389.53314153641617 val loss 381.5668287731352\n",
      "Epoch 568: train loss 389.9758314221634 val loss 382.51124427432103\n",
      "Epoch 569: train loss 390.0932768194169 val loss 378.9135248093378\n",
      "Epoch 570: train loss 393.42506970025096 val loss 389.6623335338774\n",
      "Epoch 571: train loss 396.73825935007994 val loss 390.4506647019159\n",
      "Epoch 572: train loss 396.33881745807867 val loss 389.2520475841704\n",
      "Epoch 573: train loss 401.59490215716585 val loss 399.50894237699964\n",
      "Epoch 574: train loss 407.95469143724193 val loss 398.9456936064221\n",
      "Epoch 575: train loss 405.3649170633425 val loss 391.05811273484005\n",
      "Epoch 576: train loss 394.8671988057349 val loss 384.76486133393786\n",
      "Epoch 577: train loss 393.0019567618098 val loss 384.5362984793527\n",
      "Epoch 578: train loss 391.54695627479356 val loss 381.8229355585007\n",
      "Epoch 579: train loss 389.55525721416575 val loss 381.2036881219773\n",
      "Epoch 580: train loss 389.4328216394612 val loss 379.57554117838544\n",
      "Epoch 581: train loss 389.895377677957 val loss 382.48759024483815\n",
      "Epoch 582: train loss 388.5806521084642 val loss 379.3562447684152\n",
      "Epoch 583: train loss 385.1130167116155 val loss 378.0202164422898\n",
      "Epoch 584: train loss 386.7085158590208 val loss 390.0241154261998\n",
      "Epoch 585: train loss 397.4229716562854 val loss 388.7118617466518\n",
      "Epoch 586: train loss 396.03822105289123 val loss 386.43974231538317\n",
      "Epoch 587: train loss 398.23742786466767 val loss 397.3751692998977\n",
      "Epoch 588: train loss 403.3343482931661 val loss 394.54256766183033\n",
      "Epoch 589: train loss 403.0318203466544 val loss 395.247923714774\n",
      "Epoch 590: train loss 403.969775975677 val loss 396.52659715924943\n",
      "Epoch 591: train loss 402.2279751634351 val loss 390.90077136811755\n",
      "Epoch 592: train loss 399.62946584434707 val loss 387.19019862583707\n",
      "Epoch 593: train loss 398.7767391698966 val loss 390.76131620861236\n",
      "Epoch 594: train loss 400.5073190007185 val loss 393.5420764741443\n",
      "Epoch 595: train loss 400.76927374805194 val loss 392.2117433093843\n",
      "Epoch 596: train loss 404.85069883672685 val loss 396.4085544404529\n",
      "Epoch 597: train loss 403.3893604970349 val loss 392.97196815127415\n",
      "Epoch 598: train loss 401.9260706135646 val loss 393.7511691138858\n",
      "Epoch 599: train loss 402.30659536252983 val loss 399.2490038190569\n",
      "Epoch 600: train loss 410.58839139790115 val loss 399.98968723842074\n",
      "Epoch 601: train loss 405.9533553049354 val loss 399.6889924548921\n",
      "Epoch 602: train loss 406.65235861348367 val loss 398.1213647751581\n",
      "Epoch 603: train loss 405.3285429168859 val loss 395.52939860026044\n",
      "Epoch 604: train loss 412.0679405884421 val loss 410.1544109526135\n",
      "Epoch 605: train loss 416.4011358547705 val loss 405.1435328892299\n",
      "Epoch 606: train loss 429.69466103667423 val loss 424.21285865420384\n",
      "Epoch 607: train loss 426.1236097108515 val loss 416.19046819777714\n",
      "Epoch 608: train loss 423.51144417085794 val loss 412.6648486909412\n",
      "Epoch 609: train loss 419.1294507436802 val loss 417.12666756766185\n",
      "Epoch 610: train loss 428.6827874060122 val loss 419.86975242978053\n",
      "Epoch 611: train loss 430.58005899221786 val loss 421.0031084333147\n",
      "Epoch 612: train loss 431.4369163710836 val loss 433.54436928885326\n",
      "Epoch 613: train loss 438.40106770411677 val loss 427.8456573486328\n",
      "Epoch 614: train loss 427.4720464518651 val loss 410.4203120640346\n",
      "Epoch 615: train loss 419.9081414573551 val loss 403.3534691220238\n",
      "Epoch 616: train loss 415.31179754227554 val loss 400.06634594145277\n",
      "Epoch 617: train loss 405.55252786745064 val loss 396.423457191104\n",
      "Epoch 618: train loss 406.12984430975246 val loss 394.71238890148345\n",
      "Epoch 619: train loss 423.23807363559547 val loss 417.7823195684524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m log_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(foldername, \u001b[39m'\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m os\u001b[39m.\u001b[39mmakedirs(log_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m model, history \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m     12\u001b[0m     model, \n\u001b[1;32m     13\u001b[0m     train_dataloader, \n\u001b[1;32m     14\u001b[0m     val_dataloader, \n\u001b[1;32m     15\u001b[0m     cltsp_config,\n\u001b[1;32m     16\u001b[0m     log_dir,\n\u001b[1;32m     17\u001b[0m     device\n\u001b[1;32m     18\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[99], line 83\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, train_config, log_dir, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss \u001b[39m+\u001b[39m kldivergence_loss \u001b[39m+\u001b[39m similarity_loss\n\u001b[1;32m     82\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 83\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     85\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     87\u001b[0m val_losses \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion-env/lib/python3.9/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion-env/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion-env/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion-env/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion-env/lib/python3.9/site-packages/torch/optim/adam.py:264\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m    263\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    265\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")  \n",
    "foldername = (\"./save/sine\" + \"_\" + current_time + \"/\")\n",
    "\n",
    "print('model folder:', foldername)\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "\n",
    "num_epochs = cltsp_config.n_epochs\n",
    "log_dir = os.path.join(foldername, 'results')\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "model, history = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    cltsp_config,\n",
    "    log_dir,\n",
    "    device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 16])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, dropout: float=0.1, max_seq_len: int=5000, d_model: int=512, batch_first: bool=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_first = batch_first\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(self.pe.shape)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "dim_val = 16\n",
    "dropout_pos_enc = 0.2 \n",
    "max_seq_len = 48\n",
    "n_heads = 8\n",
    "n_encoder_layers = 4\n",
    "n_features = 1\n",
    "\n",
    "positional_encoding_layer = PositionalEncoder(d_model=dim_val, dropout=dropout_pos_enc, max_seq_len=max_seq_len).to(device)\n",
    "encoder_input_layer = nn.Linear(in_features=n_features, out_features=dim_val).to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads, batch_first=True).to(device)\n",
    "encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_encoder_layers, norm=None).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([64, 48, 16])\n",
      "torch.Size([52, 48, 16])\n"
     ]
    }
   ],
   "source": [
    "for train_batch in train_dataloader:\n",
    "    seq_true = train_batch[\"time_series\"]\n",
    "    seq_true_labels = train_batch[\"labels\"]\n",
    "\n",
    "    seq_true = seq_true.to(device)\n",
    "    seq_true_labels = seq_true_labels.to(device)\n",
    "    src = encoder_input_layer(seq_true) \n",
    "    src = positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "    src = encoder(src=src)\n",
    "    print(src.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_embedding(pos, d_model=128):\n",
    "    pe = torch.zeros(pos.shape[0], pos.shape[1], d_model).to(device)\n",
    "    position = pos.unsqueeze(2)\n",
    "    div_term = 1 / torch.pow(10000.0, torch.arange(0, d_model, 2).to(device) / d_model)\n",
    "    pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 48, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_embedding(seq_true[:,:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [1.5000, 1.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((10,2))\n",
    "a[4] = torch.ones((2,))\n",
    "a + torch.ones((2,))*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
